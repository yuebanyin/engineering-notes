# LLM è¾“å‡ºè¯„ä¼°æ¡†æ¶

> è¯„ä¼°æ˜¯ LLM åº”ç”¨å·¥ç¨‹åŒ–çš„æ ¸å¿ƒéš¾é¢˜ã€‚"è¿™ä¸ªå›å¤å¥½ä¸å¥½"ä¸å†æ˜¯ä¸»è§‚åˆ¤æ–­ï¼Œè€Œæ˜¯éœ€è¦é‡åŒ–ã€å¯è¿½æº¯ã€å¯è¿­ä»£çš„è¿‡ç¨‹ã€‚è¿™ç¯‡ç¬”è®°è®°å½•äº†æˆ‘å¯¹ LLM è¯„ä¼°çš„ç†è§£å’Œå®è·µæ–¹æ³•ã€‚

---

## ğŸ¯ ä¸ºä»€ä¹ˆè¯„ä¼°å¾ˆéš¾

### LLM è¾“å‡ºçš„ç‰¹ç‚¹

| ç‰¹ç‚¹ | æŒ‘æˆ˜ |
|------|------|
| **éç¡®å®šæ€§** | åŒæ ·çš„è¾“å…¥å¯èƒ½æœ‰ä¸åŒçš„è¾“å‡º |
| **å¤šç»´åº¦** | "å¥½"æœ‰å¾ˆå¤šç§å«ä¹‰ï¼šå‡†ç¡®ã€å®Œæ•´ã€ç®€æ´ã€æ ¼å¼... |
| **ä¸»è§‚æ€§** | ä¸åŒç”¨æˆ·å¯¹"å¥½"çš„å®šä¹‰ä¸åŒ |
| **ä¸Šä¸‹æ–‡ä¾èµ–** | è„±ç¦»åœºæ™¯å¾ˆéš¾åˆ¤æ–­å¥½å |

### ä¼ ç»Ÿæµ‹è¯• vs LLM è¯„ä¼°

| ä¼ ç»Ÿæµ‹è¯• | LLM è¯„ä¼° |
|----------|----------|
| ç²¾ç¡®åŒ¹é… | è¯­ä¹‰ç›¸ä¼¼ |
| é€šè¿‡/å¤±è´¥ | ç¨‹åº¦/åˆ†æ•° |
| ç¡®å®šæ€§ | æ¦‚ç‡æ€§ |
| ä¸€æ¬¡éªŒè¯ | æŒç»­è¯„ä¼° |

---

## ğŸ“Š è¯„ä¼°ç»´åº¦æ¡†æ¶

### æ ¸å¿ƒç»´åº¦

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     æ­£ç¡®æ€§          â”‚
                    â”‚ (Correctness)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å®Œæ•´æ€§     â”‚    â”‚    æ ¼å¼      â”‚    â”‚    é£æ ¼      â”‚
â”‚(Completeness)â”‚    â”‚   (Format)   â”‚    â”‚   (Style)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å®‰å…¨æ€§     â”‚    â”‚   å®ç”¨æ€§     â”‚    â”‚   æ•ˆç‡       â”‚
â”‚  (Safety)    â”‚    â”‚ (Usefulness) â”‚    â”‚(Efficiency)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç»´åº¦è¯¦è§£

#### 1. æ­£ç¡®æ€§ï¼ˆCorrectnessï¼‰

**å®šä¹‰**ï¼šè¾“å‡ºæ˜¯å¦äº‹å®æ­£ç¡®ã€é€»è¾‘æ­£ç¡®

**å­ç»´åº¦**ï¼š
- äº‹å®å‡†ç¡®ï¼šé™ˆè¿°çš„äº‹å®æ˜¯å¦æ­£ç¡®
- é€»è¾‘ä¸€è‡´ï¼šæ¨ç†è¿‡ç¨‹æ˜¯å¦æœ‰é€»è¾‘é”™è¯¯
- æ— å¹»è§‰ï¼šæ²¡æœ‰ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯

**è¯„ä¼°æ–¹æ³•**ï¼š
```python
def evaluate_correctness(output, ground_truth=None, context=None):
    scores = {}
    
    # å¦‚æœæœ‰æ ‡å‡†ç­”æ¡ˆï¼Œæ£€æŸ¥å…³é”®ä¿¡æ¯
    if ground_truth:
        scores['factual_accuracy'] = check_key_facts(output, ground_truth)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„çŸ›ç›¾
    scores['logical_consistency'] = check_contradictions(output)
    
    # å¦‚æœæœ‰å‚è€ƒä¸Šä¸‹æ–‡ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºèŒƒå›´çš„å£°æ˜
    if context:
        scores['grounded'] = check_groundedness(output, context)
    
    return scores
```

#### 2. å®Œæ•´æ€§ï¼ˆCompletenessï¼‰

**å®šä¹‰**ï¼šæ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„æ‰€æœ‰é—®é¢˜ï¼Œè¦†ç›–äº†æ‰€æœ‰è¦ç‚¹

**è¯„ä¼°æ–¹æ³•**ï¼š
```python
def evaluate_completeness(output, requirements):
    """
    requirements: æœŸæœ›è¾“å‡ºåº”åŒ…å«çš„è¦ç‚¹åˆ—è¡¨
    """
    covered = []
    missing = []
    
    for req in requirements:
        if requirement_covered(output, req):
            covered.append(req)
        else:
            missing.append(req)
    
    return {
        'coverage_rate': len(covered) / len(requirements),
        'covered': covered,
        'missing': missing
    }
```

#### 3. æ ¼å¼ï¼ˆFormatï¼‰

**å®šä¹‰**ï¼šæ˜¯å¦ç¬¦åˆè¦æ±‚çš„è¾“å‡ºæ ¼å¼

**è¯„ä¼°æ–¹æ³•**ï¼š
```python
def evaluate_format(output, expected_format):
    """
    æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸæ ¼å¼
    """
    if expected_format == 'json':
        try:
            json.loads(output)
            return {'valid': True, 'format_score': 1.0}
        except:
            return {'valid': False, 'format_score': 0, 'error': 'Invalid JSON'}
    
    if expected_format == 'markdown':
        # æ£€æŸ¥ markdown ç»“æ„
        has_headers = bool(re.search(r'^#+\s', output, re.MULTILINE))
        has_code_blocks = '```' in output
        return {
            'has_headers': has_headers,
            'has_code_blocks': has_code_blocks,
            'format_score': (has_headers + has_code_blocks) / 2
        }
```

#### 4. å®‰å…¨æ€§ï¼ˆSafetyï¼‰

**å®šä¹‰**ï¼šä¸åŒ…å«æœ‰å®³ã€åè§ã€æ•æ„Ÿå†…å®¹

**è¯„ä¼°ç»´åº¦**ï¼š
- æœ‰å®³å†…å®¹
- åè§æ­§è§†
- éšç§æ³„éœ²
- ä¸å½“å»ºè®®

#### 5. å®ç”¨æ€§ï¼ˆUsefulnessï¼‰

**å®šä¹‰**ï¼šå¯¹ç”¨æˆ·å®Œæˆä»»åŠ¡æ˜¯å¦æœ‰å¸®åŠ©

**è¿™æ˜¯æœ€é‡è¦ä¹Ÿæœ€éš¾é‡åŒ–çš„ç»´åº¦**

---

## ğŸ”§ è¯„ä¼°æ–¹æ³•

### æ–¹æ³• 1ï¼šè§„åˆ™åŒ¹é…

æœ€ç®€å•çš„æ–¹æ³•ï¼Œé€‚åˆæœ‰æ˜ç¡®è§„åˆ™çš„åœºæ™¯ã€‚

```python
def rule_based_evaluation(output, rules):
    """
    rules: è§„åˆ™åˆ—è¡¨ï¼Œæ¯ä¸ªè§„åˆ™æ˜¯ä¸€ä¸ªæ£€æŸ¥å‡½æ•°
    """
    results = []
    
    for rule in rules:
        result = rule.check(output)
        results.append({
            'rule': rule.name,
            'passed': result.passed,
            'reason': result.reason
        })
    
    return {
        'total': len(rules),
        'passed': sum(1 for r in results if r['passed']),
        'details': results
    }

# è§„åˆ™ç¤ºä¾‹
rules = [
    Rule("é•¿åº¦æ£€æŸ¥", lambda x: 50 < len(x) < 1000),
    Rule("æ ¼å¼æ£€æŸ¥", lambda x: x.startswith('#')),
    Rule("å…³é”®è¯åŒ…å«", lambda x: 'function' in x.lower()),
    Rule("æ— ç¦ç”¨è¯", lambda x: 'æ”¿æ²»' not in x),
]
```

### æ–¹æ³• 2ï¼šç›¸ä¼¼åº¦åŒ¹é…

å½“æœ‰å‚è€ƒç­”æ¡ˆæ—¶ï¼Œæ¯”è¾ƒè¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

```python
from sentence_transformers import SentenceTransformer

def semantic_similarity(output, reference):
    """
    è®¡ç®—è¾“å‡ºä¸å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦
    """
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    embeddings = model.encode([output, reference])
    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    
    return {
        'similarity': float(similarity),
        'assessment': 'pass' if similarity > 0.8 else 'fail'
    }
```

### æ–¹æ³• 3ï¼šLLM-as-Judge

ç”¨å¦ä¸€ä¸ª LLM æ¥è¯„ä¼°è¾“å‡ºè´¨é‡ã€‚

```python
def llm_judge(output, criteria, reference=None):
    """
    ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…
    """
    prompt = f"""
    è¯·è¯„ä¼°ä»¥ä¸‹è¾“å‡ºçš„è´¨é‡ã€‚
    
    ## è¯„ä¼°æ ‡å‡†
    {criteria}
    
    ## å¾…è¯„ä¼°è¾“å‡º
    {output}
    
    {"## å‚è€ƒç­”æ¡ˆ" + reference if reference else ""}
    
    ## è¯·è¾“å‡ºè¯„ä¼°ç»“æœ
    æ ¼å¼ï¼š
    {{
      "overall_score": 1-5,
      "dimensions": {{
        "æ­£ç¡®æ€§": {{"score": 1-5, "reason": "..."}},
        "å®Œæ•´æ€§": {{"score": 1-5, "reason": "..."}},
        "æ ¼å¼": {{"score": 1-5, "reason": "..."}}
      }},
      "summary": "ä¸€å¥è¯æ€»ç»“"
    }}
    """
    
    response = llm.generate(prompt)
    return json.loads(response)
```

**LLM-as-Judge çš„æ³¨æ„äº‹é¡¹**ï¼š
- è¯„åˆ¤ LLM å¯èƒ½æœ‰è‡ªå·±çš„åè§
- éœ€è¦æ ¡å‡†ï¼ˆä¸äººå·¥è¯„ä¼°å¯¹æ¯”ï¼‰
- æˆæœ¬è€ƒè™‘ï¼ˆæ¯æ¬¡è¯„ä¼°éƒ½æ˜¯ä¸€æ¬¡ API è°ƒç”¨ï¼‰

### æ–¹æ³• 4ï¼šäººå·¥è¯„ä¼°

æœ€å¯é ä½†æœ€è´µçš„æ–¹æ³•ã€‚

```python
def human_evaluation_schema():
    """
    äººå·¥è¯„ä¼°çš„æ ‡å‡†åŒ–è¡¨å•
    """
    return {
        "evaluator_id": "string",
        "timestamp": "datetime",
        "input": "åŸå§‹è¾“å…¥",
        "output": "æ¨¡å‹è¾“å‡º",
        "ratings": {
            "correctness": {
                "score": "1-5",
                "issues": ["é—®é¢˜åˆ—è¡¨"]
            },
            "completeness": {
                "score": "1-5",
                "missing_points": ["ç¼ºå¤±çš„è¦ç‚¹"]
            },
            "format": {
                "score": "1-5",
                "issues": ["æ ¼å¼é—®é¢˜"]
            },
            "overall": {
                "score": "1-5",
                "would_use": "boolean",
                "comments": "è‡ªç”±è¯„è®º"
            }
        }
    }
```

---

## ğŸ“‹ è¯„ä¼°ä½“ç³»æ­å»º

### è¯„ä¼°æ•°æ®é›†

```python
# è¯„ä¼°æ•°æ®é›†ç»“æ„
evaluation_dataset = {
    "name": "code_generation_v1",
    "version": "1.0",
    "description": "ä»£ç ç”Ÿæˆä»»åŠ¡è¯„ä¼°é›†",
    "samples": [
        {
            "id": "001",
            "input": "ç”¨æˆ·è¾“å…¥/prompt",
            "context": "é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰",
            "expected_output": "æœŸæœ›è¾“å‡ºï¼ˆå¯é€‰ï¼‰",
            "requirements": ["è¦ç‚¹1", "è¦ç‚¹2"],
            "metadata": {
                "difficulty": "easy/medium/hard",
                "category": "åˆ†ç±»",
                "tags": ["æ ‡ç­¾"]
            }
        }
    ]
}
```

### è¯„ä¼°æµæ°´çº¿

```python
class EvaluationPipeline:
    def __init__(self, evaluators):
        self.evaluators = evaluators
    
    def evaluate(self, dataset, model):
        results = []
        
        for sample in dataset.samples:
            # ç”Ÿæˆè¾“å‡º
            output = model.generate(sample.input)
            
            # å¤šç»´åº¦è¯„ä¼°
            evaluation = {}
            for evaluator in self.evaluators:
                evaluation[evaluator.name] = evaluator.evaluate(
                    output=output,
                    sample=sample
                )
            
            results.append({
                'sample_id': sample.id,
                'input': sample.input,
                'output': output,
                'evaluation': evaluation
            })
        
        return self._aggregate_results(results)
    
    def _aggregate_results(self, results):
        # æ±‡æ€»ç»Ÿè®¡
        return {
            'total_samples': len(results),
            'average_scores': self._calculate_averages(results),
            'pass_rate': self._calculate_pass_rate(results),
            'details': results
        }
```

### æŒç»­è¯„ä¼°

```python
# è¯„ä¼°ç»“æœè¿½è¸ª
class EvaluationTracker:
    def __init__(self, storage):
        self.storage = storage
    
    def record(self, evaluation_run):
        """è®°å½•ä¸€æ¬¡è¯„ä¼°"""
        self.storage.save({
            'run_id': generate_id(),
            'timestamp': datetime.now(),
            'model_version': evaluation_run.model_version,
            'prompt_version': evaluation_run.prompt_version,
            'dataset_version': evaluation_run.dataset_version,
            'results': evaluation_run.results
        })
    
    def compare(self, run_id_a, run_id_b):
        """å¯¹æ¯”ä¸¤æ¬¡è¯„ä¼°"""
        run_a = self.storage.get(run_id_a)
        run_b = self.storage.get(run_id_b)
        
        return {
            'score_diff': run_b.avg_score - run_a.avg_score,
            'pass_rate_diff': run_b.pass_rate - run_a.pass_rate,
            'regression': self._find_regressions(run_a, run_b),
            'improvements': self._find_improvements(run_a, run_b)
        }
```

---

## ğŸ’¡ å®è·µç»éªŒ

### è®¾è®¡è¯„ä¼°é›†çš„åŸåˆ™

1. **è¦†ç›–è¾¹ç•Œæƒ…å†µ**ï¼šä¸åªæ˜¯æ­£å¸¸è¾“å…¥ï¼Œè¿˜è¦åŒ…å«æç«¯æƒ…å†µ
2. **ä»£è¡¨çœŸå®åˆ†å¸ƒ**ï¼šè¯„ä¼°é›†åº”è¯¥åæ˜ å®é™…ä½¿ç”¨åœºæ™¯
3. **å¯ç»´æŠ¤**ï¼šæ˜“äºæ›´æ–°å’Œæ‰©å±•
4. **æœ‰ç‰ˆæœ¬**ï¼šè¯„ä¼°é›†ä¹Ÿéœ€è¦ç‰ˆæœ¬ç®¡ç†

### è¯„ä¼°æŒ‡æ ‡é€‰æ‹©

| åœºæ™¯ | æ ¸å¿ƒæŒ‡æ ‡ |
|------|----------|
| ä¿¡æ¯æ£€ç´¢ | å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1 |
| ä»£ç ç”Ÿæˆ | è¯­æ³•æ­£ç¡®ç‡ã€æµ‹è¯•é€šè¿‡ç‡ |
| æ–‡æœ¬æ‘˜è¦ | ROUGEã€äº‹å®ä¸€è‡´æ€§ |
| å¯¹è¯ | è¿è´¯æ€§ã€æœ‰ç”¨æ€§ã€å®‰å…¨æ€§ |
| åˆ†ç±» | å‡†ç¡®ç‡ã€æ··æ·†çŸ©é˜µ |

### è¯„ä¼°çš„æˆæœ¬æ•ˆç›Š

```
è¯„ä¼°æ–¹æ³•æˆæœ¬å¯¹æ¯”ï¼š
è§„åˆ™æ£€æŸ¥    â”â”â”â”â”â–¸ ä½æˆæœ¬ï¼Œæœ‰é™è¦†ç›–
ç›¸ä¼¼åº¦åŒ¹é…  â”â”â”â”â”â”â”â”â”â–¸ ä¸­ç­‰æˆæœ¬ï¼Œéœ€è¦å‚è€ƒç­”æ¡ˆ
LLM-as-Judge â”â”â”â”â”â”â”â”â”â”â”â”â”â–¸ è¾ƒé«˜æˆæœ¬ï¼Œçµæ´»
äººå·¥è¯„ä¼°    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–¸ æœ€é«˜æˆæœ¬ï¼Œæœ€å¯é 
```

**å»ºè®®**ï¼š
- å¼€å‘é˜¶æ®µï¼šè§„åˆ™ + LLM-as-Judge
- å‘å¸ƒå‰ï¼šåŠ å…¥äººå·¥æŠ½æ£€
- çº¿ä¸Šç›‘æ§ï¼šè‡ªåŠ¨åŒ–è§„åˆ™ + é‡‡æ ·äººå·¥

### é¿å…è¯„ä¼°é™·é˜±

| é™·é˜± | è¯´æ˜ | é¿å…æ–¹æ³• |
|------|------|----------|
| è¿‡æ‹Ÿåˆè¯„ä¼°é›† | é’ˆå¯¹è¯„ä¼°é›†ä¼˜åŒ–ï¼Œä½†æ³›åŒ–å·® | ä¿ç•™æµ‹è¯•é›†ï¼Œå®šæœŸæ›´æ¢ |
| æŒ‡æ ‡æ¸¸æˆ | ä¼˜åŒ–æŒ‡æ ‡ä½†å®é™…ä½“éªŒæ²¡æå‡ | ç»“åˆäººå·¥è¯„ä¼° |
| è¯„ä¼°åè§ | è¯„ä¼°æ–¹æ³•æœ‰ç³»ç»Ÿæ€§åè§ | å¤šç»´åº¦ã€å¤šæ–¹æ³•äº¤å‰éªŒè¯ |
| æˆæœ¬å¤±æ§ | è¯„ä¼°æœ¬èº«æ¶ˆè€—å¤ªå¤šèµ„æº | åˆ†å±‚è¯„ä¼°ï¼Œé‡‡æ ·ç­–ç•¥ |

---

## ğŸ”„ è¯„ä¼°é©±åŠ¨è¿­ä»£

```
å®šä¹‰è¯„ä¼°æ ‡å‡†
      â”‚
      â–¼
æ„å»ºè¯„ä¼°æ•°æ®é›†
      â”‚
      â–¼
è¿è¡ŒåŸºçº¿è¯„ä¼° â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                      â”‚
      â–¼                      â”‚
åˆ†æé—®é¢˜æ¨¡å¼                 â”‚
      â”‚                      â”‚
      â–¼                      â”‚
ä¼˜åŒ–ï¼ˆprompt/æ¨¡å‹/å·¥å…·ï¼‰     â”‚
      â”‚                      â”‚
      â–¼                      â”‚
é‡æ–°è¯„ä¼° â”€â”€â”€â”€â”¬â”€â”€ æ”¹è¿› â”€â”€â”€â”€â”€â”€â–ºâ”‚
             â”‚               â”‚
             â””â”€â”€ å›å½’ â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š ç›¸å…³ç¬”è®°

- [æç¤ºè¯å·¥ç¨‹åŸºç¡€](../prompting/prompt-engineering-fundamentals.md)
- [LLM API ä½¿ç”¨æœ€ä½³å®è·µ](../llm/llm-api-best-practices.md)
- [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](../troubleshooting/common-llm-issues.md)

---

## å‚è€ƒèµ„æ–™

- [OpenAI Evals](https://github.com/openai/evals)
- [LangChain Evaluation](https://python.langchain.com/docs/guides/evaluation/)
- [RAGAS - RAG Assessment](https://github.com/explodinggradients/ragas)
- [LLM-as-Judge Paper](https://arxiv.org/abs/2306.05685)
