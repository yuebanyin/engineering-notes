# LLM API ä½¿ç”¨æœ€ä½³å®è·µ

> ä½œä¸ºå‰ç«¯å¼€å‘è€…ï¼Œä½¿ç”¨ LLM ä¸»è¦æ˜¯é€šè¿‡ API è°ƒç”¨ã€‚è¿™ç¯‡ç¬”è®°è®°å½•äº† API ä½¿ç”¨ä¸­çš„å…³é”®å‚æ•°ã€é”™è¯¯å¤„ç†ã€æˆæœ¬æ§åˆ¶ç­‰å®è·µç»éªŒã€‚

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### API è°ƒç”¨çš„åŸºæœ¬ç»“æ„

```javascript
// å…¸å‹çš„ LLM API è°ƒç”¨
const response = await client.chat.completions.create({
  // æ¨¡å‹é€‰æ‹©
  model: "gpt-4",
  
  // æ¶ˆæ¯å†å²
  messages: [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Hello!" }
  ],
  
  // ç”Ÿæˆå‚æ•°
  temperature: 0.7,
  max_tokens: 1000,
  
  // é«˜çº§é€‰é¡¹
  stream: false,
  tools: [],
});
```

### å…³é”®å‚æ•°è¯¦è§£

#### Temperatureï¼ˆæ¸©åº¦ï¼‰

æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚

| å€¼ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|---|------|----------|
| 0 | å‡ ä¹ç¡®å®šæ€§ | ä»£ç ç”Ÿæˆã€äº‹å®é—®ç­” |
| 0.3-0.5 | ä½éšæœºæ€§ | æ–‡æ¡£ç”Ÿæˆã€æ‘˜è¦ |
| 0.7-0.9 | ä¸­ç­‰éšæœºæ€§ | åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´ |
| 1.0+ | é«˜éšæœºæ€§ | åˆ›æ„æ¢ç´¢ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰ |

```javascript
// ä»£ç ç”Ÿæˆï¼šç”¨ä½æ¸©åº¦
const codeResponse = await client.chat.completions.create({
  model: "gpt-4",
  temperature: 0.2,  // ä½æ¸©åº¦ï¼Œç¡®ä¿ä»£ç ç¨³å®š
  messages: [/* ... */]
});

// åˆ›æ„å»ºè®®ï¼šç”¨è¾ƒé«˜æ¸©åº¦
const ideaResponse = await client.chat.completions.create({
  model: "gpt-4",
  temperature: 0.8,  // è¾ƒé«˜æ¸©åº¦ï¼Œå¢åŠ å¤šæ ·æ€§
  messages: [/* ... */]
});
```

#### Max Tokensï¼ˆæœ€å¤§è¾“å‡ºé•¿åº¦ï¼‰

é™åˆ¶è¾“å‡ºçš„ token æ•°é‡ã€‚

```javascript
// éœ€è¦é•¿è¾“å‡ºçš„åœºæ™¯
const longResponse = await client.chat.completions.create({
  max_tokens: 4000,  // å…è®¸é•¿è¾“å‡º
  // ...
});

// çŸ­å›å¤åœºæ™¯
const shortResponse = await client.chat.completions.create({
  max_tokens: 100,   // é™åˆ¶è¾“å‡ºé•¿åº¦
  // ...
});
```

**æ³¨æ„**ï¼š
- max_tokens æ˜¯è¾“å‡ºé™åˆ¶ï¼Œä¸åŒ…æ‹¬è¾“å…¥
- è¾¾åˆ°é™åˆ¶ä¼šçªç„¶æˆªæ–­ï¼Œä¸ä¸€å®šæ˜¯å®Œæ•´çš„å¥å­
- éœ€è¦ä¼°ç®—å¹¶ç•™è¶³ä½™é‡

#### Top Pï¼ˆæ ¸é‡‡æ ·ï¼‰

å¦ä¸€ç§æ§åˆ¶éšæœºæ€§çš„æ–¹å¼ï¼Œé€šå¸¸ä¸ temperature äºŒé€‰ä¸€ã€‚

```javascript
// é€šå¸¸ä¸åŒæ—¶è°ƒæ•´ temperature å’Œ top_p
const response = await client.chat.completions.create({
  temperature: 1,     // é»˜è®¤
  top_p: 0.9,         // åªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡å‰ 90% çš„ token
  // ...
});
```

---

## ğŸ“‹ æ¶ˆæ¯ç»“æ„è®¾è®¡

### è§’è‰²ç±»å‹

```javascript
const messages = [
  // system: è®¾å®š AI çš„è¡Œä¸ºè§„åˆ™
  {
    role: "system",
    content: `ä½ æ˜¯ä¸€ä¸ªä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œæ“…é•¿å‘ç° JavaScript ä»£ç ä¸­çš„é—®é¢˜ã€‚
    
    è§„åˆ™ï¼š
    - æ¯æ¬¡åªæŒ‡å‡ºæœ€å…³é”®çš„ 3 ä¸ªé—®é¢˜
    - æä¾›å…·ä½“çš„ä¿®æ”¹å»ºè®®
    - ç”¨ä¸­æ–‡å›å¤`
  },
  
  // user: ç”¨æˆ·è¾“å…¥
  {
    role: "user",
    content: "è¯·å®¡æŸ¥è¿™æ®µä»£ç ï¼š..."
  },
  
  // assistant: æ¨¡å‹ä¹‹å‰çš„å›å¤ï¼ˆå¤šè½®å¯¹è¯ï¼‰
  {
    role: "assistant",
    content: "è¿™æ®µä»£ç æœ‰ä»¥ä¸‹é—®é¢˜..."
  },
  
  // tool: å·¥å…·è°ƒç”¨çš„ç»“æœ
  {
    role: "tool",
    tool_call_id: "call_123",
    content: '{"result": "success"}'
  }
];
```

### System Prompt è®¾è®¡

```javascript
// âœ… ç»“æ„åŒ–çš„ System Prompt
const systemPrompt = `
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±å‰ç«¯å¼€å‘ä¸“å®¶ï¼Œä¸“æ³¨äº React å’Œ TypeScriptã€‚

# èƒ½åŠ›
- ä»£ç å®¡æŸ¥å’Œä¼˜åŒ–å»ºè®®
- æ¶æ„è®¾è®¡å’¨è¯¢
- æ€§èƒ½é—®é¢˜è¯Šæ–­

# è§„åˆ™
1. ç”¨ä¸­æ–‡å›å¤
2. ä»£ç ç¤ºä¾‹ä½¿ç”¨ TypeScript
3. å¦‚æœé—®é¢˜è¶…å‡ºèƒ½åŠ›èŒƒå›´ï¼Œè¯šå®è¯´æ˜

# è¾“å‡ºæ ¼å¼
å¯¹äºä»£ç é—®é¢˜ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
## é—®é¢˜
[é—®é¢˜æè¿°]

## å»ºè®®
[è§£å†³æ–¹æ¡ˆ]

## ä»£ç 
\`\`\`typescript
[ä¿®æ”¹åçš„ä»£ç ]
\`\`\`
`;
```

### å¤šè½®å¯¹è¯ç®¡ç†

```javascript
class ConversationManager {
  constructor(systemPrompt, maxHistory = 20) {
    this.systemPrompt = systemPrompt;
    this.maxHistory = maxHistory;
    this.history = [];
  }
  
  addUserMessage(content) {
    this.history.push({ role: "user", content });
    this._trimHistory();
  }
  
  addAssistantMessage(content) {
    this.history.push({ role: "assistant", content });
    this._trimHistory();
  }
  
  getMessages() {
    return [
      { role: "system", content: this.systemPrompt },
      ...this.history
    ];
  }
  
  _trimHistory() {
    // ä¿ç•™æœ€è¿‘çš„ N è½®å¯¹è¯
    if (this.history.length > this.maxHistory * 2) {
      this.history = this.history.slice(-this.maxHistory * 2);
    }
  }
  
  estimateTokens() {
    // ç²—ç•¥ä¼°ç®— token æ•°é‡
    const text = this.systemPrompt + 
      this.history.map(m => m.content).join('');
    return Math.ceil(text.length / 4);  // è‹±æ–‡çº¦ 4 å­—ç¬¦ä¸€ä¸ª token
  }
}
```

---

## ğŸ”§ æµå¼å“åº”

### ä¸ºä»€ä¹ˆä½¿ç”¨æµå¼

- **æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ**ï¼šç”¨æˆ·ç«‹å³çœ‹åˆ°è¾“å‡ºå¼€å§‹
- **æ›´å¿«çš„æ„ŸçŸ¥é€Ÿåº¦**ï¼šé¦–å­—èŠ‚æ—¶é—´ < å®Œæ•´å“åº”æ—¶é—´
- **é•¿è¾“å‡ºå‹å¥½**ï¼šä¸éœ€è¦ç­‰å¾…å®Œæ•´ç”Ÿæˆ

### å®ç°ç¤ºä¾‹

```javascript
// Node.js ç¯å¢ƒ
async function streamChat(messages) {
  const stream = await client.chat.completions.create({
    model: "gpt-4",
    messages,
    stream: true,
  });
  
  let fullContent = '';
  
  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content || '';
    fullContent += content;
    
    // å®æ—¶è¾“å‡º
    process.stdout.write(content);
  }
  
  return fullContent;
}

// æµè§ˆå™¨ç¯å¢ƒ + React
function useChatStream() {
  const [content, setContent] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  
  const sendMessage = async (messages) => {
    setIsLoading(true);
    setContent('');
    
    const response = await fetch('/api/chat', {
      method: 'POST',
      body: JSON.stringify({ messages }),
    });
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const chunk = decoder.decode(value);
      setContent(prev => prev + chunk);
    }
    
    setIsLoading(false);
  };
  
  return { content, isLoading, sendMessage };
}
```

---

## âš ï¸ é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯ç±»å‹

```javascript
async function callWithErrorHandling(messages) {
  try {
    return await client.chat.completions.create({
      model: "gpt-4",
      messages,
    });
  } catch (error) {
    // é€Ÿç‡é™åˆ¶
    if (error.status === 429) {
      const retryAfter = error.headers?.['retry-after'] || 60;
      throw new RateLimitError(`Rate limited. Retry after ${retryAfter}s`);
    }
    
    // ä¸Šä¸‹æ–‡è¿‡é•¿
    if (error.code === 'context_length_exceeded') {
      throw new ContextLengthError('Input too long. Please reduce content.');
    }
    
    // æ¨¡å‹ä¸å¯ç”¨
    if (error.status === 503) {
      throw new ModelUnavailableError('Model temporarily unavailable.');
    }
    
    // API Key é—®é¢˜
    if (error.status === 401) {
      throw new AuthenticationError('Invalid API key.');
    }
    
    // å…¶ä»–é”™è¯¯
    throw new LLMError(`API Error: ${error.message}`);
  }
}
```

### é‡è¯•ç­–ç•¥

```javascript
async function callWithRetry(fn, options = {}) {
  const {
    maxRetries = 3,
    baseDelay = 1000,
    maxDelay = 60000,
    retryableErrors = [429, 503, 500]
  } = options;
  
  let lastError;
  
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      
      // æ£€æŸ¥æ˜¯å¦å¯é‡è¯•
      if (!retryableErrors.includes(error.status)) {
        throw error;
      }
      
      // è®¡ç®—å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ + æŠ–åŠ¨ï¼‰
      const delay = Math.min(
        baseDelay * Math.pow(2, attempt) + Math.random() * 1000,
        maxDelay
      );
      
      console.log(`Attempt ${attempt + 1} failed. Retrying in ${delay}ms...`);
      await sleep(delay);
    }
  }
  
  throw lastError;
}

// ä½¿ç”¨
const response = await callWithRetry(() => 
  client.chat.completions.create({ model: "gpt-4", messages })
);
```

---

## ğŸ’° æˆæœ¬æ§åˆ¶

### Token ä¼°ç®—

```javascript
// ç®€å•ä¼°ç®—ï¼ˆç²—ç•¥ï¼‰
function estimateTokens(text) {
  // è‹±æ–‡çº¦ 4 å­—ç¬¦ä¸€ä¸ª token
  // ä¸­æ–‡çº¦ 1.5 å­—ç¬¦ä¸€ä¸ª token
  const hasChineseChars = /[\u4e00-\u9fa5]/.test(text);
  return Math.ceil(text.length / (hasChineseChars ? 1.5 : 4));
}

// ä½¿ç”¨ tiktoken ç²¾ç¡®è®¡ç®—ï¼ˆæ¨èï¼‰
import { encoding_for_model } from 'tiktoken';

function countTokens(text, model = 'gpt-4') {
  const enc = encoding_for_model(model);
  const tokens = enc.encode(text);
  enc.free();
  return tokens.length;
}
```

### æˆæœ¬è®¡ç®—

```javascript
// å„æ¨¡å‹ä»·æ ¼ï¼ˆç¤ºä¾‹ï¼Œè¯·æŸ¥é˜…æœ€æ–°å®šä»·ï¼‰
const PRICING = {
  'gpt-4': { input: 0.03, output: 0.06 },          // per 1K tokens
  'gpt-4-turbo': { input: 0.01, output: 0.03 },
  'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 },
  'claude-3-opus': { input: 0.015, output: 0.075 },
  'claude-3-sonnet': { input: 0.003, output: 0.015 },
};

function estimateCost(inputTokens, outputTokens, model) {
  const pricing = PRICING[model];
  if (!pricing) return null;
  
  return {
    inputCost: (inputTokens / 1000) * pricing.input,
    outputCost: (outputTokens / 1000) * pricing.output,
    totalCost: (inputTokens / 1000) * pricing.input + 
               (outputTokens / 1000) * pricing.output
  };
}
```

### æˆæœ¬ä¼˜åŒ–ç­–ç•¥

| ç­–ç•¥ | è¯´æ˜ | èŠ‚çœæ¯”ä¾‹ |
|------|------|----------|
| æ¨¡å‹é™çº§ | ç®€å•ä»»åŠ¡ç”¨ 3.5 è€Œé 4 | 50-90% |
| å‹ç¼© prompt | ç²¾ç®€ system prompt | 10-30% |
| ç¼“å­˜å“åº” | ç›¸åŒè¾“å…¥å¤ç”¨ç»“æœ | å˜åŒ–å¤§ |
| é™åˆ¶è¾“å‡º | è®¾ç½®åˆç†çš„ max_tokens | 10-50% |
| æ‰¹é‡å¤„ç† | åˆå¹¶å¤šä¸ªè¯·æ±‚ | API è´¹ç”¨ä¸å˜ï¼Œä½†å‡å°‘å¼€é”€ |

```javascript
// å“åº”ç¼“å­˜ç¤ºä¾‹
class LLMCache {
  constructor(storage, ttl = 3600) {
    this.storage = storage;
    this.ttl = ttl;
  }
  
  async get(key) {
    const cached = await this.storage.get(key);
    if (cached && Date.now() - cached.timestamp < this.ttl * 1000) {
      return cached.response;
    }
    return null;
  }
  
  async set(key, response) {
    await this.storage.set(key, {
      response,
      timestamp: Date.now()
    });
  }
  
  getCacheKey(messages, model, temperature) {
    // å¯¹äº temperature=0 çš„ç¡®å®šæ€§è¯·æ±‚ï¼Œå¯ä»¥ç¼“å­˜
    if (temperature > 0) return null;
    return crypto
      .createHash('md5')
      .update(JSON.stringify({ messages, model }))
      .digest('hex');
  }
}
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿—

### è¯·æ±‚æ—¥å¿—

```javascript
class LLMLogger {
  async logRequest(request, response, metadata = {}) {
    const log = {
      timestamp: new Date().toISOString(),
      requestId: generateId(),
      
      // è¯·æ±‚ä¿¡æ¯
      model: request.model,
      inputTokens: response.usage.prompt_tokens,
      outputTokens: response.usage.completion_tokens,
      
      // æ€§èƒ½ä¿¡æ¯
      latency: metadata.latency,
      
      // æˆæœ¬ä¿¡æ¯
      cost: estimateCost(
        response.usage.prompt_tokens,
        response.usage.completion_tokens,
        request.model
      ).totalCost,
      
      // ä¸šåŠ¡ä¿¡æ¯
      feature: metadata.feature,
      userId: metadata.userId,
    };
    
    await this.save(log);
  }
  
  async getStats(timeRange) {
    const logs = await this.query(timeRange);
    
    return {
      totalRequests: logs.length,
      totalTokens: sum(logs, l => l.inputTokens + l.outputTokens),
      totalCost: sum(logs, l => l.cost),
      avgLatency: average(logs, l => l.latency),
      modelDistribution: groupBy(logs, l => l.model),
    };
  }
}
```

### ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ | æè¿° | å‘Šè­¦é˜ˆå€¼å»ºè®® |
|------|------|-------------|
| è¯·æ±‚å»¶è¿Ÿ | API å“åº”æ—¶é—´ | P99 > 30s |
| é”™è¯¯ç‡ | å¤±è´¥è¯·æ±‚æ¯”ä¾‹ | > 5% |
| Token ä½¿ç”¨é‡ | æ—¥/æœˆ token æ¶ˆè€— | æ¥è¿‘é…é¢ 80% |
| æˆæœ¬ | æ—¥/æœˆè´¹ç”¨ | è¶…é¢„ç®— |
| 429 é¢‘ç‡ | é€Ÿç‡é™åˆ¶è§¦å‘æ¬¡æ•° | > 10/min |

---

## ğŸ’¡ å®è·µç»éªŒ

### æ¨¡å‹é€‰æ‹©ç­–ç•¥

```javascript
function selectModel(task) {
  const modelMatrix = {
    // ç®€å•ä»»åŠ¡ï¼šç”¨ä¾¿å®œå¿«é€Ÿçš„æ¨¡å‹
    'simple_qa': 'gpt-3.5-turbo',
    'classification': 'gpt-3.5-turbo',
    'extraction': 'gpt-3.5-turbo',
    
    // å¤æ‚ä»»åŠ¡ï¼šç”¨å¼ºå¤§çš„æ¨¡å‹
    'code_generation': 'gpt-4',
    'complex_reasoning': 'gpt-4',
    'creative_writing': 'gpt-4',
    
    // é•¿æ–‡æœ¬ï¼šç”¨å¤§ä¸Šä¸‹æ–‡æ¨¡å‹
    'document_analysis': 'gpt-4-turbo',
    'long_summarization': 'claude-3-sonnet',
  };
  
  return modelMatrix[task] || 'gpt-3.5-turbo';
}
```

### è¯·æ±‚ä¼˜åŒ–æ¸…å•

- [ ] æ˜¯å¦å¯ä»¥ç¼“å­˜ï¼Ÿï¼ˆæ¸©åº¦=0 ä¸”è¾“å…¥ç¨³å®šï¼‰
- [ ] æ˜¯å¦å¯ä»¥ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹ï¼Ÿ
- [ ] System prompt æ˜¯å¦ç²¾ç®€ï¼Ÿ
- [ ] æ˜¯å¦éœ€è¦å®Œæ•´çš„å†å²å¯¹è¯ï¼Ÿ
- [ ] max_tokens æ˜¯å¦è®¾ç½®åˆç†ï¼Ÿ
- [ ] æ˜¯å¦éœ€è¦æµå¼å“åº”ï¼Ÿ

---

## ğŸ“š ç›¸å…³ç¬”è®°

- [æç¤ºè¯å·¥ç¨‹åŸºç¡€](../prompting/prompt-engineering-fundamentals.md)
- [ä¸Šä¸‹æ–‡çª—å£ç®¡ç†](context-window-management.md)
- [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](../troubleshooting/common-llm-issues.md)

---

## å‚è€ƒèµ„æ–™

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic API Documentation](https://docs.anthropic.com/)
- [tiktoken](https://github.com/openai/tiktoken)
